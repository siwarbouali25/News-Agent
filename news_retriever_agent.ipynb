{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siwarbouali25/News-Agent/blob/siwar-bouali/news_retriever_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e09c44e",
      "metadata": {
        "id": "4e09c44e"
      },
      "source": [
        "# News Retriever Agent (Dataset-only)\n",
        "\n",
        "This notebook provides:\n",
        "\n",
        "- **Category preview**: show the N most recent articles in selected categories.\n",
        "- **Grounded Q&A**: answer questions using ONLY retrieved context from your local dataset.\n",
        "\n",
        "> Minimal, robust, and Colab-ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8cd5ffd8",
      "metadata": {
        "id": "8cd5ffd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbc2bf0-40f5-4e7e-fcc9-81915be1ae74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install \"transformers>=4.44\" accelerate bitsandbytes langchain langchain-community dateparser\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNEAgevfJm04",
        "outputId": "4d793d9f-cf83-47c1-c447-15d8238b3b1f"
      },
      "id": "LNEAgevfJm04",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U --prefer-binary bitsandbytes"
      ],
      "metadata": {
        "id": "pr8864ErKSvt"
      },
      "id": "pr8864ErKSvt",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLWjfDCTK_R0",
        "outputId": "9e2ae16d-de61-45a5-b3e7-b0357015bc54"
      },
      "id": "GLWjfDCTK_R0",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ac19b0",
      "metadata": {
        "id": "f7ac19b0"
      },
      "source": [
        "## 1) Load your dataset\n",
        "- If you already have a DataFrame `df` in memory, **skip this cell**.\n",
        "- Otherwise, set `DATA_PATH` and the text column (`TEXT_COL`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aad77717",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "aad77717",
        "outputId": "bd3d4f37-a0a7-49c0-9f97-f1b431888325"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/merged_articles (5).csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2858438112.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/merged_articles (5).csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# If `df` already exists in memory, you can skip this cell.\n",
        "DATA_PATH =  \"/content/merged_articles (5).csv\"\n",
        "TEXT_COL  = \"content\"  # or \"text\"\n",
        "\n",
        "if DATA_PATH:\n",
        "    if DATA_PATH.endswith(\".csv\"):\n",
        "        df = pd.read_csv(DATA_PATH)\n",
        "    elif DATA_PATH.endswith(\".jsonl\") or DATA_PATH.endswith(\".json\"):\n",
        "        df = pd.read_json(DATA_PATH, lines=True)\n",
        "    else:\n",
        "        raise ValueError(\"Use a .csv or .jsonl file, or provide df beforehand.\")\n",
        "\n",
        "if 'df' in globals():\n",
        "    print(\"Columns:\", list(df.columns))\n",
        "    print(\"Rows:\", len(df))\n",
        "else:\n",
        "    print(\"No DataFrame named `df` found. Provide it or set DATA_PATH.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2244b66",
      "metadata": {
        "id": "b2244b66"
      },
      "source": [
        "## 2) Normalize dates, ensure schema, and de-duplicate URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29de0fc2",
      "metadata": {
        "id": "29de0fc2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import timezone\n",
        "\n",
        "ALLOWED_CATEGORIES = [\"Politics\",\"World\",\"Science\",\"Health\",\"Sports\",\"Entertainment\",\"Culture\",\"Society\",\"Technology\"]\n",
        "\n",
        "# Ensure text column\n",
        "assert any(c in df.columns for c in [\"content\",\"text\"]), \"Need a text column named 'content' or 'text'.\"\n",
        "if \"content\" not in df.columns and \"text\" in df.columns:\n",
        "    df[\"content\"] = df[\"text\"]\n",
        "\n",
        "# Ensure category column exists (pre-labeled)\n",
        "assert \"category\" in df.columns, \"Your df needs a 'category' column with values from ALLOWED_CATEGORIES.\"\n",
        "\n",
        "# Parse dates into timezone-aware UTC\n",
        "DATE_COL = \"published_date\"\n",
        "if DATE_COL in df.columns:\n",
        "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\", utc=True)\n",
        "else:\n",
        "    # Create an empty date column if missing\n",
        "    df[DATE_COL] = pd.NaT\n",
        "\n",
        "# Keep ISO string for vectorstore metadata (safer to serialize)\n",
        "df[\"published_date_iso\"] = df[DATE_COL].dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\").str.replace(r\"(\\+0000)$\", \"+00:00\", regex=True)\n",
        "\n",
        "# Optional: de-duplicate by URL\n",
        "if \"url\" in df.columns:\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
        "    print(f\"Deduped by URL: {before} ‚Üí {len(df)}\")\n",
        "else:\n",
        "    print(\"No 'url' column found; skipping URL dedupe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97d1fcf",
      "metadata": {
        "id": "d97d1fcf"
      },
      "source": [
        "## 3) Build FAISS retriever (MMR for diversity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d5efb76",
      "metadata": {
        "id": "0d5efb76"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "TEXT_COL = \"content\"\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=120)\n",
        "\n",
        "texts, metas = [], []\n",
        "for i, r in df.iterrows():\n",
        "    txt = str(r.get(TEXT_COL, \"\")).strip()\n",
        "    if not txt:\n",
        "        continue\n",
        "    for j, ch in enumerate(splitter.split_text(txt)):\n",
        "        texts.append(ch)\n",
        "        metas.append({\n",
        "            \"id\": str(r.get(\"id\", i)),\n",
        "            \"title\": str(r.get(\"title\", \"\")),\n",
        "            \"url\": str(r.get(\"url\", \"\")),\n",
        "            \"published_date\": r.get(\"published_date_iso\"),\n",
        "            \"source\": str(r.get(\"source\", \"\")),\n",
        "            \"category\": str(r.get(\"category\", \"\")),\n",
        "            \"chunk\": j,\n",
        "        })\n",
        "\n",
        "print(f\"Chunks: {len(texts)}\")\n",
        "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = FAISS.from_texts(texts=texts, embedding=emb, metadatas=metas)\n",
        "\n",
        "# MMR retriever (less duplicate chunks)\n",
        "retriever = vectordb.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 12, \"fetch_k\": 60, \"lambda_mult\": 0.5}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22cc91df",
      "metadata": {
        "id": "22cc91df"
      },
      "source": [
        "## 4) Utilities ‚Äî dates, dedupe, category preview, retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f7e518",
      "metadata": {
        "id": "49f7e518"
      },
      "outputs": [],
      "source": [
        "def meta_to_timestamp_utc(meta_val):\n",
        "    try:\n",
        "        return pd.to_datetime(meta_val, utc=True)\n",
        "    except Exception:\n",
        "        return pd.NaT\n",
        "\n",
        "def sort_hits_newest_first(hits):\n",
        "    hits.sort(key=lambda d: (meta_to_timestamp_utc(d.metadata.get(\"published_date\")) or pd.Timestamp(0, tz=timezone.utc)), reverse=True)\n",
        "    return hits\n",
        "\n",
        "def dedupe_by_url(hits):\n",
        "    seen, uniq = set(), []\n",
        "    for h in hits:\n",
        "        key = h.metadata.get(\"url\") or h.metadata.get(\"id\")\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        uniq.append(h)\n",
        "    return uniq\n",
        "\n",
        "def filter_hits_by_categories(hits, categories):\n",
        "    if not categories:\n",
        "        return hits\n",
        "    allowed = set(categories)\n",
        "    return [h for h in hits if h.metadata.get(\"category\") in allowed]\n",
        "\n",
        "def retrieve_for_query(query, categories=None, k=12):\n",
        "    hits = retriever.invoke(query)\n",
        "    hits = filter_hits_by_categories(hits, categories or [])\n",
        "    hits = dedupe_by_url(hits)\n",
        "    hits = sort_hits_newest_first(hits)\n",
        "    return hits[:k]\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "import pandas as pd\n",
        "\n",
        "def _canonical_source(row):\n",
        "    s = (row.get(\"source\") or \"\").strip()\n",
        "    if s:\n",
        "        return s\n",
        "    # fallback to URL domain if source missing\n",
        "    url = (row.get(\"url\") or \"\").strip()\n",
        "    if url:\n",
        "        try:\n",
        "            host = urlparse(url).netloc.lower()\n",
        "            # collapse common subdomains\n",
        "            if host.startswith(\"www.\"):\n",
        "                host = host[4:]\n",
        "            return host or \"unknown\"\n",
        "        except Exception:\n",
        "            pass\n",
        "    return \"unknown\"\n",
        "\n",
        "# Build a stable normalized source column once\n",
        "df[\"source_norm\"] = df.apply(_canonical_source, axis=1)\n",
        "\n",
        "def get_recent_articles_diverse(categories, n_per_cat=5, per_source_limit=1, pretty=True):\n",
        "    \"\"\"\n",
        "    For each category, return the N most recent articles,\n",
        "    limiting picks to `per_source_limit` items per source (default 1).\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    cats = [c for c in categories if c in ALLOWED_CATEGORIES]\n",
        "\n",
        "    for c in cats:\n",
        "        sub = df[df[\"category\"] == c].copy()\n",
        "\n",
        "        # newest first\n",
        "        sub = sub.sort_values(\"published_date\", ascending=False, na_position=\"last\")\n",
        "\n",
        "        # enforce per-source cap\n",
        "        if per_source_limit == 1:\n",
        "            # simplest: keep the most recent per source\n",
        "            sub = sub.drop_duplicates(subset=[\"source_norm\"], keep=\"first\")\n",
        "        else:\n",
        "            # cap >1: take top-k per source then re-sort globally\n",
        "            sub = (sub\n",
        "                   .groupby(\"source_norm\", group_keys=False)\n",
        "                   .head(per_source_limit)\n",
        "                   .sort_values(\"published_date\", ascending=False, na_position=\"last\"))\n",
        "\n",
        "        rows = sub.head(n_per_cat)[[\"title\",\"url\",\"published_date\",\"source\",\"source_norm\"]].to_dict(orient=\"records\")\n",
        "        results[c] = rows\n",
        "\n",
        "    if pretty:\n",
        "        for cat, items in results.items():\n",
        "            print(f\"\\n=== {cat} ‚Äî {len(items)} most recent (diverse sources) ===\")\n",
        "            for r in items:\n",
        "                dt = r.get(\"published_date\")\n",
        "                date_str = dt.strftime(\"%Y-%m-%d %H:%M UTC\") if pd.notna(dt) else \"Unknown date\"\n",
        "                shown_source = r.get(\"source\") or r.get(\"source_norm\")\n",
        "                print(f\"- {r['title']} ({shown_source})\")\n",
        "                print(f\"  {r['url']}  [{date_str}]\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9710ef60",
      "metadata": {
        "id": "9710ef60"
      },
      "source": [
        "## 5) Load Llama 3.1 8B Instruct (4-bit)\n",
        "> You must have accepted the model license on Hugging Face and be logged in in this runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd93a22f",
      "metadata": {
        "id": "cd93a22f"
      },
      "outputs": [],
      "source": [
        "# If needed:\n",
        "!pip -q install huggingface_hub\n",
        "!huggingface-cli login  # paste your hf_ token\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  # long context (license-gated)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # use torch.bfloat16 on A100 if preferred\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "mdl = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "print(\"Model loaded on:\", mdl.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f160d3",
      "metadata": {
        "id": "49f160d3"
      },
      "source": [
        "## 6) Build the dataset-only answerer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba8b83f",
      "metadata": {
        "id": "4ba8b83f"
      },
      "outputs": [],
      "source": [
        "SYSTEM_MSG = (\n",
        "  \"You are the Dataset Retrieval Agent for a private news corpus. \"\n",
        "  \"Use ONLY the provided CONTEXT from the local dataset. \"\n",
        "  \"If the answer is not present, reply exactly: \\\"I don't know.\\\" \"\n",
        "  \"Be concise, neutral, and always cite article titles and URLs.\"\n",
        ")\n",
        "\n",
        "def pack_context_unique(docs, max_ctx_tokens=6000):\n",
        "    parts, used, seen = [], 0, set()\n",
        "    for d in docs:\n",
        "        key = d.metadata.get(\"url\") or d.metadata.get(\"id\")\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        block = (\n",
        "            f\"Title: {d.metadata.get('title','')}\\n\"\n",
        "            f\"URL: {d.metadata.get('url','')}\\n\"\n",
        "            f\"Snippet: {d.page_content}\\n\\n\"\n",
        "        )\n",
        "        n = len(tok(block).input_ids)\n",
        "        if used + n > max_ctx_tokens:\n",
        "            break\n",
        "        parts.append(block); used += n\n",
        "    return \"\".join(parts) if parts else \"NO_MATCH\"\n",
        "\n",
        "def build_llama_prompt(question: str, context: str):\n",
        "    messages = [\n",
        "        {\"role\":\"system\", \"content\": SYSTEM_MSG},\n",
        "        {\"role\":\"user\",   \"content\":\n",
        "            f\"CONTEXT:\\n{context}\\n\\n\"\n",
        "            f\"QUESTION: {question}\\n\\n\"\n",
        "            \"Respond with a concise factual answer. Cite sources like: (Title ‚Äî URL). \"\n",
        "            \"If multiple sources agree, cite up to 2. If not answerable from CONTEXT, reply exactly: \\\"I don't know.\\\"\"\n",
        "        }\n",
        "    ]\n",
        "    return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def answer_from_dataset(question: str, docs, max_ctx_tokens=6000, max_new_tokens=220):\n",
        "    context = pack_context_unique(docs, max_ctx_tokens=max_ctx_tokens)\n",
        "    prompt  = build_llama_prompt(question, context)\n",
        "    enc = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=16384).to(mdl.device)\n",
        "    out = mdl.generate(\n",
        "        **enc,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tok.eos_token_id\n",
        "    )\n",
        "    return tok.decode(out[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c9ed2f4",
      "metadata": {
        "id": "3c9ed2f4"
      },
      "source": [
        "## 7) Unified `ask()` ‚Äî with or without categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb1331f",
      "metadata": {
        "id": "dcb1331f"
      },
      "outputs": [],
      "source": [
        "def ask(question: str, categories: list | None = None, k: int = 12,\n",
        "        max_ctx_tokens: int = 6000, max_new_tokens: int = 220, return_docs: bool = False):\n",
        "    \"\"\"    Ask a question over your dataset.\n",
        "    - If `categories` is None or empty, search the WHOLE dataset.\n",
        "    - Otherwise, restrict retrieval to the selected categories.\n",
        "    - Returns a concise, cited answer. If not answerable from context: 'I don't know.'\n",
        "    \"\"\"\n",
        "    hits = retrieve_for_query(question, categories=categories or [], k=k)\n",
        "    if not hits:\n",
        "        ans = \"I don't know.\"\n",
        "        return (ans, hits) if return_docs else ans\n",
        "    ans = answer_from_dataset(question, hits, max_ctx_tokens=max_ctx_tokens, max_new_tokens=max_new_tokens)\n",
        "    return (ans, hits) if return_docs else ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d77dd2a4",
      "metadata": {
        "id": "d77dd2a4"
      },
      "source": [
        "## 8) Demo ‚Äî most recent by category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3aafc3",
      "metadata": {
        "id": "0c3aafc3"
      },
      "outputs": [],
      "source": [
        "user_categories = [\"Technology\", \"Politics\"]  # change as you like\n",
        "get_recent_articles_diverse(user_categories, n_per_cat=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install streamlit pyngrok pandas\n"
      ],
      "metadata": {
        "id": "ARe0YAKPRGni"
      },
      "id": "ARe0YAKPRGni",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_AUTH_TOKEN = \"2w3oMNyM0oQ5tjatgGWLlCrepJ5_4VRybodh39hzX9fcub3gL\"  # <-- paste token here (keep it secret!)\n",
        "assert NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN.startswith(\"\"), \"Set NGROK_AUTH_TOKEN\"\n"
      ],
      "metadata": {
        "id": "9CTpU-rYRO7z"
      },
      "id": "9CTpU-rYRO7z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "ALLOWED_CATEGORIES = [\n",
        "    \"Politics\",\"World\",\"Science\",\"Health\",\"Sports\",\n",
        "    \"Entertainment\",\"Culture\",\"Society\",\"Technology\"\n",
        "]\n",
        "DATE_COL = \"published_date\"\n",
        "TEXT_COL_CANDIDATES = [\"content\",\"text\",\"body\"]\n",
        "\n",
        "st.set_page_config(page_title=\"News Retriever (Dataset-only)\", layout=\"wide\")\n",
        "st.title(\"üì∞ News Retriever ‚Äî Category Preview\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "@st.cache_data(show_spinner=False)\n",
        "def load_df(file) -> pd.DataFrame:\n",
        "    if file is None:\n",
        "        return pd.DataFrame()\n",
        "    name = file.name.lower()\n",
        "    if name.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file)\n",
        "    elif name.endswith(\".jsonl\") or name.endswith(\".json\"):\n",
        "        df = pd.read_json(file, lines=True)\n",
        "    else:\n",
        "        st.error(\"Please upload .csv or .jsonl/.json\")\n",
        "        return pd.DataFrame()\n",
        "    return df\n",
        "\n",
        "def ensure_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df.empty: return df\n",
        "\n",
        "    # text column\n",
        "    text_col = None\n",
        "    for c in TEXT_COL_CANDIDATES:\n",
        "        if c in df.columns:\n",
        "            text_col = c; break\n",
        "    if text_col is None:\n",
        "        df[\"content\"] = \"\"\n",
        "    elif text_col != \"content\":\n",
        "        df[\"content\"] = df[text_col]\n",
        "\n",
        "    # category\n",
        "    if \"category\" not in df.columns:\n",
        "        df[\"category\"] = \"\"\n",
        "\n",
        "    # basic columns\n",
        "    if \"url\" not in df.columns: df[\"url\"] = \"\"\n",
        "    if \"source\" not in df.columns: df[\"source\"] = \"\"\n",
        "\n",
        "    # dates (UTC)\n",
        "    if DATE_COL not in df.columns:\n",
        "        df[DATE_COL] = pd.NaT\n",
        "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\", utc=True)\n",
        "\n",
        "    # normalized source for diversity\n",
        "    df[\"source_norm\"] = df.apply(_canonical_source, axis=1)\n",
        "\n",
        "    # drop duplicate URLs\n",
        "    if \"url\" in df.columns:\n",
        "        df = df.drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def _canonical_source(row):\n",
        "    s = str(row.get(\"source\") or \"\").strip()\n",
        "    if s:\n",
        "        return s\n",
        "    url = str(row.get(\"url\") or \"\").strip()\n",
        "    if url:\n",
        "        try:\n",
        "            host = urlparse(url).netloc.lower()\n",
        "            if host.startswith(\"www.\"):\n",
        "                host = host[4:]\n",
        "            return host or \"unknown\"\n",
        "        except Exception:\n",
        "            pass\n",
        "    return \"unknown\"\n",
        "\n",
        "def pretty_date(dt):\n",
        "    if pd.isna(dt): return \"Unknown date\"\n",
        "    try:\n",
        "        return dt.strftime(\"%Y-%m-%d %H:%M UTC\")\n",
        "    except Exception:\n",
        "        return \"Unknown date\"\n",
        "\n",
        "def get_recent_articles_diverse(df: pd.DataFrame, categories, n_per_cat=5, per_source_limit=1):\n",
        "    results = {}\n",
        "    cats = [c for c in categories if c in ALLOWED_CATEGORIES]\n",
        "    for c in cats:\n",
        "        sub = df[df[\"category\"] == c].copy()\n",
        "        if sub.empty:\n",
        "            results[c] = []; continue\n",
        "\n",
        "        # newest ‚Üí oldest\n",
        "        sub = sub.sort_values(DATE_COL, ascending=False, na_position=\"last\")\n",
        "\n",
        "        # enforce max per source\n",
        "        if per_source_limit == 1:\n",
        "            sub = sub.drop_duplicates(subset=[\"source_norm\"], keep=\"first\")\n",
        "        else:\n",
        "            sub = (\n",
        "                sub.groupby(\"source_norm\", group_keys=False)\n",
        "                   .head(per_source_limit)\n",
        "                   .sort_values(DATE_COL, ascending=False, na_position=\"last\")\n",
        "            )\n",
        "\n",
        "        results[c] = sub.head(n_per_cat)[[\"title\",\"url\",DATE_COL,\"source\",\"source_norm\"]].to_dict(orient=\"records\")\n",
        "    return results\n",
        "\n",
        "def article_card(item: dict):\n",
        "    title = item.get(\"title\") or \"(Untitled)\"\n",
        "    url   = item.get(\"url\") or \"\"\n",
        "    src   = item.get(\"source\") or item.get(\"source_norm\") or \"unknown\"\n",
        "    dt    = item.get(DATE_COL)\n",
        "\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "        <div style=\"border:1px solid #e9ecef;border-radius:14px;padding:16px;height:100%\">\n",
        "          <div style=\"font-weight:600;font-size:1.05rem;line-height:1.3;margin-bottom:6px\">{title}</div>\n",
        "          <div style=\"color:#6c757d;font-size:0.9rem;margin-bottom:8px\">{src} ‚Ä¢ {pretty_date(dt)}</div>\n",
        "          <a href=\"{url}\" target=\"_blank\" style=\"text-decoration:none\">Open article ‚Üó</a>\n",
        "        </div>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "# ---------- Sidebar ----------\n",
        "with st.sidebar:\n",
        "    st.header(\"Upload dataset\")\n",
        "    up = st.file_uploader(\"CSV or JSONL/JSON\", type=[\"csv\",\"jsonl\",\"json\"])\n",
        "    st.caption(\"Needs: category, title, url, and a text column (content/text). Optional: published_date, source.\")\n",
        "\n",
        "    st.header(\"Filters\")\n",
        "    chosen = st.multiselect(\"Categories\", ALLOWED_CATEGORIES, default=[\"Technology\",\"Politics\"])\n",
        "    n_per_cat = st.slider(\"Articles per category\", 1, 10, 5)\n",
        "    per_source_limit = st.slider(\"Max per source (diversity)\", 1, 3, 1)\n",
        "\n",
        "    show_table = st.checkbox(\"Show raw table\", value=False)\n",
        "\n",
        "# ---------- Main ----------\n",
        "df = load_df(up)\n",
        "if df.empty:\n",
        "    st.info(\"Upload a dataset to begin.\")\n",
        "    st.stop()\n",
        "\n",
        "df = ensure_schema(df)\n",
        "\n",
        "if not chosen:\n",
        "    st.warning(\"Choose at least one category.\")\n",
        "    st.stop()\n",
        "\n",
        "results = get_recent_articles_diverse(df, categories=chosen, n_per_cat=n_per_cat, per_source_limit=per_source_limit)\n",
        "\n",
        "for cat in chosen:\n",
        "    items = results.get(cat, [])\n",
        "    st.subheader(f\"{cat} ‚Äî {len(items)} most recent (diverse sources)\")\n",
        "    if not items:\n",
        "        st.caption(\"No articles found.\")\n",
        "        continue\n",
        "\n",
        "    cols = st.columns(3)\n",
        "    for i, item in enumerate(items):\n",
        "        with cols[i % 3]:\n",
        "            article_card(item)\n",
        "\n",
        "    if show_table:\n",
        "        st.dataframe(pd.DataFrame(items))\n"
      ],
      "metadata": {
        "id": "taCs1v-GRYR6"
      },
      "id": "taCs1v-GRYR6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, time, re, os, sys\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill old tunnels/servers\n",
        "ngrok.kill()\n",
        "!pkill -f \"streamlit run app.py\" || true\n",
        "\n",
        "# Start Streamlit (background)\n",
        "port = 8501\n",
        "proc = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", str(port), \"--server.address\", \"0.0.0.0\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "# ngrok tunnel\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public_url = ngrok.connect(port, \"http\").public_url\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Optional: stream a few lines of Streamlit logs to know it's up\n",
        "for _ in range(10):\n",
        "    line = proc.stdout.readline()\n",
        "    if not line: break\n",
        "    print(line.strip())\n",
        "time.sleep(2)\n",
        "print(\"‚úÖ Open the URL above in your browser.\")\n"
      ],
      "metadata": {
        "id": "uuy4tYDBRaxs"
      },
      "id": "uuy4tYDBRaxs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"siwarbouali25\"\n",
        "!git config --global user.email \"siwar.bouali@esprit.tn\"\n"
      ],
      "metadata": {
        "id": "2GpWqHOoB8H0"
      },
      "id": "2GpWqHOoB8H0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/your-username/your-repo-name.git\n"
      ],
      "metadata": {
        "id": "hjGlt-NSCCvE"
      },
      "id": "hjGlt-NSCCvE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}